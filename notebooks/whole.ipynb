{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "507bd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## 2. Data Processing & Feature Engineering (`src/data_processing.py`)\n",
    "\n",
    "### Purpose\n",
    "\n",
    "#Turn raw transactions into **customer-level, model-ready features**.\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "\"* Fail early if data is broken \"\n",
    "\"One function = one responsibility No hard-coded magic\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "RAW_PATH = Path(r\"c:\\Users\\hp\\Pictures\\Bati Bank\\Credit-Risk-Probability-Model-for-Alternative-Data\\data\\raw\\CreditRisk-data.csv\")\n",
    "PROCESSED_PATH = Path(r\"C:\\Users\\hp\\Pictures\\Bati Bank\\Credit-Risk-Probability-Model-for-Alternative-Data\\data\\process\\processed.csv\")\n",
    "\n",
    "\n",
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Safely load raw transaction data.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, parse_dates=[\"TransactionStartTime\"])\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Loaded dataset is empty\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_aggregates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create customer-level aggregate features.\"\"\"\n",
    "    try:\n",
    "        agg = df.groupby(\"CustomerId\").agg(\n",
    "            total_amount=(\"Amount\", \"sum\"),\n",
    "            avg_amount=(\"Amount\", \"mean\"),\n",
    "            txn_count=(\"TransactionId\", \"count\"),\n",
    "            amount_std=(\"Amount\", \"std\"),\n",
    "        ).reset_index()\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Missing required column: {e}\")\n",
    "\n",
    "    agg[\"amount_std\"] = agg[\"amount_std\"].fillna(0)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def extract_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract time-based behavioral features.\"\"\"\n",
    "    df = df.copy()\n",
    "    ts = df[\"TransactionStartTime\"]\n",
    "\n",
    "    df[\"txn_hour\"] = ts.dt.hour\n",
    "    df[\"txn_day\"] = ts.dt.day\n",
    "    df[\"txn_month\"] = ts.dt.month\n",
    "    df[\"txn_year\"] = ts.dt.year\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_features() -> pd.DataFrame:\n",
    "    \"\"\"End-to-end feature generation.\"\"\"\n",
    "    df = load_data(RAW_PATH)\n",
    "    df = extract_time_features(df)\n",
    "\n",
    "    agg = create_aggregates(df)\n",
    "\n",
    "    final_df = pd.merge(df, agg, on=\"CustomerId\", how=\"left\")\n",
    "\n",
    "    final_df.to_csv(PROCESSED_PATH, index=False)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_features()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34021ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering (Customer-Level)\n",
    "#We aggregate transactions into customer behavior.\n",
    "def create_customer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create customer-level behavioral features\"\"\"\n",
    "\n",
    "    customer_agg = (\n",
    "        df.groupby(\"CustomerId\")\n",
    "        .agg(\n",
    "            total_amount=(\"Amount\", \"sum\"),\n",
    "            avg_amount=(\"Amount\", \"mean\"),\n",
    "            txn_count=(\"TransactionId\", \"count\"),\n",
    "            amount_std=(\"Amount\", \"std\"),\n",
    "            fraud_txn_count=(\"FraudResult\", \"sum\"),\n",
    "            avg_value=(\"Value\", \"mean\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    customer_agg[\"amount_std\"] = customer_agg[\"amount_std\"].fillna(0)\n",
    "\n",
    "    return customer_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3f3ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Behavior Features\n",
    "def extract_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"txn_hour\"] = df[\"TransactionStartTime\"].dt.hour\n",
    "    df[\"txn_day\"] = df[\"TransactionStartTime\"].dt.day\n",
    "    df[\"txn_month\"] = df[\"TransactionStartTime\"].dt.month\n",
    "\n",
    "    return (\n",
    "        df.groupby(\"CustomerId\")\n",
    "        .agg(\n",
    "            avg_txn_hour=(\"txn_hour\", \"mean\"),\n",
    "            avg_txn_day=(\"txn_day\", \"mean\"),\n",
    "            active_months=(\"txn_month\", \"nunique\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "287f2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Proxy Target Engineering with RFM (`Task 4`)\n",
    "\n",
    "### Step 1: Compute RFM Metrics\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def compute_rfm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    snapshot_date = df[\"TransactionStartTime\"].max() + pd.Timedelta(days=1)\n",
    "\n",
    "    rfm = (\n",
    "        df.groupby(\"CustomerId\")\n",
    "        .agg(\n",
    "            recency=(\"TransactionStartTime\",\n",
    "                     lambda x: (snapshot_date - x.max()).days),\n",
    "            frequency=(\"TransactionId\", \"count\"),\n",
    "            monetary=(\"Amount\", \"sum\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return rfm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92ef2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Clustering & Risk Label\n",
    "def assign_high_risk_label(rfm: pd.DataFrame) -> pd.DataFrame:\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(\n",
    "        rfm[[\"recency\", \"frequency\", \"monetary\"]]\n",
    "    )\n",
    "\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    rfm[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    cluster_summary = rfm.groupby(\"cluster\").mean()\n",
    "\n",
    "    # Least engaged = high risk\n",
    "    high_risk_cluster = cluster_summary[\"frequency\"].idxmin()\n",
    "\n",
    "    rfm[\"is_high_risk\"] = (\n",
    "        rfm[\"cluster\"] == high_risk_cluster\n",
    "    ).astype(int)\n",
    "\n",
    "    return rfm[[\"CustomerId\", \"is_high_risk\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fe371b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Training Table Assembly\n",
    "def build_training_table():\n",
    "    df = load_raw_data()\n",
    "\n",
    "    customer_features = create_customer_features(df)\n",
    "    time_features = extract_time_features(df)\n",
    "    rfm = compute_rfm(df)\n",
    "    target = assign_high_risk_label(rfm)\n",
    "\n",
    "    final_df = (\n",
    "        customer_features\n",
    "        .merge(time_features, on=\"CustomerId\")\n",
    "        .merge(target, on=\"CustomerId\")\n",
    "    )\n",
    "\n",
    "    final_df.to_csv(\n",
    "        PROCESSED_PATH / \"training_data.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ee071f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Model Training with MLflow (`src/train.py`)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def train_models(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[\"CustomerId\", \"is_high_risk\"])\n",
    "    y = df[\"is_high_risk\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2,\n",
    "        random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        with mlflow.start_run(run_name=name):\n",
    "            model.fit(X_train, y_train)\n",
    "            probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            auc = roc_auc_score(y_test, probs)\n",
    "\n",
    "            mlflow.log_metric(\"roc_auc\", auc)\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "            print(f\"{name} AUC: {auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093701fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged run_id: 3953ce7f103841a68639c9c6bf5db1a8\n"
     ]
    }
   ],
   "source": [
    "## 5. FastAPI Inference Service (`src/api/main.py`)\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import sklearn  # If you are using sklearn models\n",
    "\n",
    "# Start logging a new run\n",
    "with mlflow.start_run() as run:\n",
    "    # Log parameters, metrics, or artifacts\n",
    "    mlflow.log_param(\"param1\", 5)\n",
    "    mlflow.log_metric(\"metric1\", np.random.random())\n",
    "    # Example: Log a model if you have one\n",
    "    # mlflow.sklearn.log_model(your_model, \"CreditRiskModel\")  # Replace with your model\n",
    "    \n",
    "print(\"Logged run_id:\", run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3330fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             run_id    status\n",
      "0  3953ce7f103841a68639c9c6bf5db1a8  FINISHED\n",
      "1  a2cbc2233aca4376a9989b6d6a16e67e  FINISHED\n",
      "2  95c21a309088466ba6a8ea1d946a3d08  FINISHED\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Specify your experiment name or ID\n",
    "experiment_name = \"CreditRiskModel\"  # Adjust if necessary\n",
    "\n",
    "# Get the experiment\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Search for runs in that experiment\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "print(runs[['run_id', 'status']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "278d88c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 20:21:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Registered model 'CreditRiskModel' already exists. Creating a new version of this model...\n",
      "2025/12/16 20:22:07 WARNING mlflow.tracking._model_registry.fluent: Run with id a171f8dceccb4e3bbc3aa6e12e087007 has no artifacts at artifact path 'model', registering model based on models:/m-6411348b8c8b4c73aa8f4089341636fa instead\n",
      "Created version '3' of model 'CreditRiskModel'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create an example model\n",
    "X, y = make_classification(n_samples=100, n_features=10)\n",
    "model = LogisticRegression()\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    model.fit(X, y)\n",
    "    mlflow.sklearn.log_model(model, \"model\")  # Ensure this name matches your loading path\n",
    "    \n",
    "    # Optionally register the model\n",
    "    mlflow.register_model(f\"runs:/{run.info.run_id}/model\", \"CreditRiskModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ddca5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 20:24:21 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Registered model 'CreditRiskModel' already exists. Creating a new version of this model...\n",
      "2025/12/16 20:24:32 WARNING mlflow.tracking._model_registry.fluent: Run with id aa37041378e54609bb656d84a65052ca has no artifacts at artifact path 'model', registering model based on models:/m-5a3c9ea4797f40648cf8db4d708d1247 instead\n",
      "Created version '4' of model 'CreditRiskModel'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create and train a logistic regression model\n",
    "X, y = make_classification(n_samples=100, n_features=10)\n",
    "model = LogisticRegression()\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    model.fit(X, y)\n",
    "    # Ensure you log the model with the expected path\n",
    "    mlflow.sklearn.log_model(model, \"model\")  # This should be the exact name you will load later\n",
    "\n",
    "    # Register the model\n",
    "    mlflow.register_model(f\"runs:/{run.info.run_id}/model\", \"CreditRiskModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ee248fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.sklearn.load_model(\"models:/CreditRiskModel/latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3470ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'CreditRiskModel' already exists. Creating a new version of this model...\n",
      "2025/12/16 20:26:45 WARNING mlflow.tracking._model_registry.fluent: Run with id aa37041378e54609bb656d84a65052ca has no artifacts at artifact path 'model', registering model based on models:/m-5a3c9ea4797f40648cf8db4d708d1247 instead\n",
      "Created version '5' of model 'CreditRiskModel'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1765906005464, current_stage='None', deployment_job_state=None, description=None, last_updated_timestamp=1765906005464, metrics=None, model_id=None, name='CreditRiskModel', params=None, run_id='aa37041378e54609bb656d84a65052ca', run_link=None, source='models:/m-5a3c9ea4797f40648cf8db4d708d1247', status='READY', status_message=None, tags={}, user_id=None, version=5>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.register_model(f\"runs:/{run.info.run_id}/model\", name=\"CreditRiskModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bca3ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = mlflow.sklearn.load_model(\"models:/CreditRiskModel/latest\")\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    print(f\"Failed to load model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e583bd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RunData: metrics={}, params={}, tags={'mlflow.runName': 'colorful-bat-612',\n",
      " 'mlflow.source.name': 'c:\\\\Users\\\\hp\\\\Pictures\\\\Bati '\n",
      "                       'Bank\\\\Credit-Risk-Probability-Model-for-Alternative-Data\\\\.week4\\\\Lib\\\\site-packages\\\\ipykernel_launcher.py',\n",
      " 'mlflow.source.type': 'LOCAL',\n",
      " 'mlflow.user': 'hp'}>\n"
     ]
    }
   ],
   "source": [
    "run_data = mlflow.get_run(\"a171f8dceccb4e3bbc3aa6e12e087007\")\n",
    "print(run_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "adbff400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "#Unit Testing (tests/test_data_processing.py)\n",
    "# Unit Testing (tests/test_data_processing.py)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Adjust the path depending on your project structure\n",
    "project_path = r\"C:\\Users\\hp\\Pictures\\Bati Bank\\Credit-Risk-Probability-Model-for-Alternative-Data\"\n",
    "# Add the src directory to the path\n",
    "src_path = os.path.join(project_path, 'src')\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "# Now you can import your function\n",
    "from data_processing import create_aggregates  # Adjust if you have sub-modules\n",
    "\n",
    "# Example test function\n",
    "def test_create_aggregates():\n",
    "    df = pd.DataFrame({\n",
    "        \"CustomerId\": [1, 1, 2],\n",
    "        \"Amount\": [100, 200, 50],\n",
    "        \"TransactionId\": [1, 2, 3]\n",
    "    })\n",
    "\n",
    "    result = create_aggregates(df)\n",
    "    \n",
    "    assert \"total_amount\" in result.columns\n",
    "    assert result.loc[result.CustomerId == 1, \"total_amount\"].iloc[0] == 300\n",
    "\n",
    "# Run the test\n",
    "test_create_aggregates()\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b34f5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Full Feature Pipeline with Encoding, Scaling, and WoE (`src/data_processing.py`)\n",
    "#Below is a **complete production-grade pipeline**. You can run this file end-to-end and it will output a clean training table.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "RAW_PATH = Path(r\"C:\\Users\\hp\\Pictures\\Bati Bank\\Credit-Risk-Probability-Model-for-Alternative-Data\\data\\raw\\CreditRisk-data.csv\")\n",
    "OUT_PATH = Path(r\"C:\\Users\\hp\\Pictures\\Bati Bank\\Credit-Risk-Probability-Model-for-Alternative-Data\\data\\process/training_data.csv\")\n",
    "\n",
    "\n",
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing data file: {path}\")\n",
    "    df = pd.read_csv(path, parse_dates=[\"TransactionStartTime\"])\n",
    "    if df.isnull().all().any():\n",
    "        raise ValueError(\"One or more columns are fully null\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    ts = df[\"TransactionStartTime\"]\n",
    "    df[\"txn_hour\"] = ts.dt.hour\n",
    "    df[\"txn_day\"] = ts.dt.day\n",
    "    df[\"txn_month\"] = ts.dt.month\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_customer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    agg = df.groupby(\"CustomerId\").agg(\n",
    "        total_amount=(\"Amount\", \"sum\"),\n",
    "        avg_amount=(\"Amount\", \"mean\"),\n",
    "        txn_count=(\"TransactionId\", \"count\"),\n",
    "        amount_std=(\"Amount\", \"std\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    agg[\"amount_std\"] = agg[\"amount_std\"].fillna(0)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def build_training_table() -> pd.DataFrame:\n",
    "    df = load_data(RAW_PATH)\n",
    "    df = add_time_features(df)\n",
    "    agg = aggregate_customer(df)\n",
    "\n",
    "    df = df.merge(agg, on=\"CustomerId\", how=\"left\")\n",
    "    df.to_csv(OUT_PATH, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_training_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43f1eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. RFM Target Creation Script (`src/rfm_target.py`)\n",
    "#This script **creates the proxy default label** and persists it.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def build_rfm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    snapshot = df[\"TransactionStartTime\"].max() + pd.Timedelta(days=1)\n",
    "\n",
    "    rfm = df.groupby(\"CustomerId\").agg(\n",
    "        recency=(\"TransactionStartTime\", lambda x: (snapshot - x.max()).days),\n",
    "        frequency=(\"TransactionId\", \"count\"),\n",
    "        monetary=(\"Amount\", \"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    return rfm\n",
    "\n",
    "\n",
    "def label_high_risk(rfm: pd.DataFrame) -> pd.DataFrame:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(rfm[[\"recency\", \"frequency\", \"monetary\"]])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    rfm[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "    summary = rfm.groupby(\"cluster\").mean()\n",
    "    high_risk_cluster = summary[\"frequency\"].idxmin()\n",
    "\n",
    "    rfm[\"is_high_risk\"] = (rfm[\"cluster\"] == high_risk_cluster).astype(int)\n",
    "    return rfm[[\"CustomerId\", \"is_high_risk\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "274554df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Model Training with Multiple Models (`src/train.py`)\n",
    "#This trains **Logistic Regression and Gradient Boosting**, logs both, and selects the best.\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def train_and_log(X, y, model, name):\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        model.fit(X, y)\n",
    "        preds = model.predict_proba(X)[:, 1]\n",
    "        auc = roc_auc_score(y, preds)\n",
    "        mlflow.log_metric(\"roc_auc\", auc)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        return auc\n",
    "\n",
    "\n",
    "def main(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[\"is_high_risk\"])\n",
    "    y = df[\"is_high_risk\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "    auc_lr = train_and_log(X_train, y_train, lr, \"LogisticRegression\")\n",
    "    auc_gb = train_and_log(X_train, y_train, gb, \"GradientBoosting\")\n",
    "\n",
    "    print(f\"LR AUC: {auc_lr:.3f}, GB AUC: {auc_gb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2918771",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Credit Score Scaling (Probability â†’ Score)\n",
    "#Industry-standard scaling:\n",
    "\n",
    "def probability_to_score(pd, base_score=600, pdo=50):\n",
    "    odds = pd / (1 - pd)\n",
    "    factor = pdo / np.log(2)\n",
    "    offset = base_score - factor * np.log(1)\n",
    "    return offset - factor * np.log(odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa243b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12. Loan Amount & Duration Heuristic\n",
    "\n",
    "def recommend_loan(score):\n",
    "    if score >= 700:\n",
    "        return {\"amount\": 50000, \"months\": 12}\n",
    "    elif score >= 600:\n",
    "        return {\"amount\": 20000, \"months\": 6}\n",
    "    else:\n",
    "        return {\"amount\": 5000, \"months\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9d96eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13. FastAPI with Validation (`src/api/pydantic_models.py`)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class CustomerFeatures(BaseModel):\n",
    "    total_amount: float\n",
    "    avg_amount: float\n",
    "    txn_count: int\n",
    "    amount_std: float\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    risk_probability: float\n",
    "    credit_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0bd887d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FastAPI Inference\n",
    "#src/api/main.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model = mlflow.sklearn.load_model(\n",
    "    \"models:/CreditRiskModel/latest\"\n",
    ")\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(features: dict):\n",
    "    try:\n",
    "        df = pd.DataFrame([features])\n",
    "        prob = model.predict_proba(df)[0, 1]\n",
    "        score = probability_to_score(prob)\n",
    "        loan = recommend_loan(score)\n",
    "\n",
    "        return {\n",
    "            \"risk_probability\": prob,\n",
    "            \"credit_score\": score,\n",
    "            \"loan_offer\": loan\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(400, str(e))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".week4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
